#!/bin/bash

#
# Staring the hdfs datanode process directly works fine for some cases. If you
# only ever start containers from the slave image directly, this bootstrap
# script is unnecessary.
#
# If, however, you start a container from the slave image, and then commit it,
# the image that results from the commit will have a VERSION file already. If
# you subsequently start multiple containers from *that* image, they'll all
# have the same datanodeUuid. If those slave containers all try to hook up to
# the same master, only one of them will do so sucessfully, because the master
# node will think that the slave nodes are all trying to share out the same
# underlying storage.
#
# In order to work around this, we need to set the datanodeUuid to a new,
# unique value on startup if we haven't already. This ensures that each
# datanode in a cluster can successfully hook up to the master.
#
# In order to maintain the same datanodeUuid across container stops and starts,
# we only update the uuid if we haven't already. This is detected by inserting
# a special comment in the first line of the VERSION file.
#
# Note, however, that as currently written, you'll have problems if you commit
# a container that has already been made unique. The VERSION file will already
# have the unique marker, and will accordingly leave it as-is on container
# startup of that new image.
#
# If you really need to support that use, your best bet is probably to make
# the unique marker something derived from the image name and search the whole
# VERSION file for it instead of looking at the first line.
#

VERSION_PATH=/tmp/hadoop-hdfs/dfs/data/current/VERSION
UNIQUE_MARKER="#UNIQUE_DATANODE"

make_unique() {
	version_path="$1"
	
	sed -i -e "/datanodeUuid=/s:\(=\).*:\1$(uuidgen):" \
		-e "1 i $UNIQUE_MARKER" "$version_path"
}

is_unique() {
	version_path="$1"
	
	first_line=$(head -1 "$version_path")
	if [[ $first_line = $UNIQUE_MARKER ]]; then
		return 0
	fi
	return 1
}


if [[ -f "$VERSION_PATH" ]] && ! is_unique "$VERSION_PATH"; then
	make_unique "$VERSION_PATH"
fi

chown -R hdfs:hdfs /tmp/hadoop-hdfs

exec hdfs datanode
